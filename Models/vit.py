# -*- coding: utf-8 -*-
"""Vit

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1w3gWJOXx9rmUTSjXvpHLxdaAfwKsFrat
"""

import numpy as np
from collections import defaultdict
import matplotlib.pyplot as plt
from torchinfo import summary
from einops import rearrange, repeat
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from torchvision import datasets, transforms
import torchvision
from einops import rearrange, repeat
from einops.layers.torch import Rearrange

def pair(t):
    return t if isinstance(t, tuple) else (t, t)

# classes

class PreNorm(nn.Module):
    def __init__(self, dim, fn):
        super().__init__()
        self.norm = nn.LayerNorm(dim)
        self.fn = fn
    def forward(self, x, **kwargs):
        return self.fn(self.norm(x), **kwargs)
    
# define the FeedForward class

class FeedForward(nn.Module):
    def __init__(self, dim, hidden_dim, dropout = 0.):
        super().__init__()
        self.net = nn.Sequential(
            #nn.Linear(dim, hidden_dim),
            nn.Conv1d(in_channels=dim, out_channels=hidden_dim,kernel_size=1, padding=0),
            nn.GELU(),
            nn.Dropout(dropout),
            #nn.Linear(hidden_dim, dim),
            nn.Conv1d(in_channels=hidden_dim, out_channels=dim,kernel_size=1, padding=0),
            nn.Dropout(dropout)
        )
    def forward(self, x):
        return self.net(x.transpose(1,2)).transpose(1,2)

# define the Attention class

class Attention(nn.Module):
    def __init__(self, dim,shape, heads = 8, dim_head = 64, dropout = 0.):
        super().__init__()
        inner_dim = dim_head *  heads
        project_out = not (heads == 1 and dim_head == dim)
        self.shape=shape
        self.heads = heads
        self.dim_head = dim_head
        self.scale = dim_head ** -0.5
        self.attend = nn.Softmax(dim = -1)
        # Convolutional layers to project the input into query, key, and value vectors
        self.to_keys = nn.Conv1d(in_channels=dim, out_channels=inner_dim, kernel_size=1,bias=False)
        self.to_queries = nn.Conv1d(in_channels=dim, out_channels=inner_dim,kernel_size= 1,bias=False)
        self.to_values = nn.Conv1d(in_channels=dim, out_channels=inner_dim, kernel_size=1,bias=False)
        self.to_out = nn.Sequential(
            nn.Linear(inner_dim, dim),
            nn.Dropout(dropout)
        ) if project_out else nn.Identity()

        self.out1= nn.Flatten(start_dim=1,end_dim=2)
    def forward(self, x):
        b, p, d = x.shape # 128,257,32
        # step 1: define the keys,values and queries
        # Reshape the queries, keys, and values for the multi-head attention
        keys = self.to_keys(x.transpose(1,2)).transpose(1,2).contiguous().view(b, self.heads,p, self.dim_head) # B(batch size) x H (Number of heads) x S (number of patches -> sequence length) x D (head dimension)
        #print("keys",keys.shape)
        values = self.to_values(x.transpose(1,2)).transpose(1,2).contiguous().view(b, self.heads,p, self.dim_head)
        queries = self.to_queries(x.transpose(1,2)).transpose(1,2).contiguous().view(b, self.heads,p, self.dim_head) #B x H x S x D
        #q=map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h = self.heads),queries)

        q,k,v = queries,keys,values
         # Compute the dot product between queries and keys for each head and  Scale the dot product by the square root of the dimensionality of the key vector
        dots = torch.matmul(q,k.transpose(-1,-2)) * self.scale # B x H x S x S
        # Apply the softmax function to obtain the attention weights
        att = self.attend(dots)
        # Compute the weighted sum of the values using the attention weights for each head
        out = torch.matmul(att,v)  # B x H x S x D

               # B x S x H x D -> B x S x E
        #print("before attention",out.shape)
       
        attn_output = out.transpose(1, 2).contiguous().view(b, p, d)  # B x S x H x D -> B x S x E
        #print(out.shape)
        #print("after attention",attn_output.shape)
        #  Project the concatenated output into the output dimension using a convolutional layer
        return self.to_out(attn_output)


    @staticmethod
    def get_indices(h, w):
        y = torch.arange(h, dtype=torch.long)
        x = torch.arange(w, dtype=torch.long)
        
        y1, x1, y2, x2 = torch.meshgrid(y, x, y, x, indexing='ij')
        indices = (y1 - y2 + h - 1) * (2 * w - 1) + x1 - x2 + w - 1
        indices = indices.flatten()
        
        return indices
    
 # define the Transformer class -> Transformer block : Attention and Feedforward functions

class Transformer(nn.Module):
    def __init__(self, dim, depth, heads, dim_head, mlp_dim,shape, dropout = 0.):
        super().__init__()
        self.layers = nn.ModuleList([])
        #self.multiheadattention = MultiheadAttentionConv2d(embed_dim=dim,num_heads=heads, dropout=dropout)
        for _ in range(depth):
            self.layers.append(nn.ModuleList([
                PreNorm(dim,Attention(dim=32,shape=(16,16), heads = 4, dim_head = 8, dropout = 0.)),#, in_channels=dim,out_channels=dim,heads = heads, dim_head = dim_head, dropout = dropout
                PreNorm(dim, FeedForward(dim, mlp_dim, dropout = dropout))
            ]))
    def forward(self, x):
        for attn, ff in self.layers:
            #print(x.shape)
            x = attn(x) + x
            x = ff(x) + x
        return x
    
# define the ViT class:

class ViT(nn.Module):
    def __init__(self,  image_size, patch_size, num_classes, dim, depth, heads,mlp_dim,  pool = 'cls', channels = 3, dim_head = 64, dropout = 0., emb_dropout = 0.):
        super().__init__()
        image_height, image_width = pair(image_size)
        reduced_size = image_size // patch_size
        shape = (reduced_size, reduced_size)
        self.dim=dim
        patch_height, patch_width = pair(patch_size)

        assert image_height % patch_height == 0 and image_width % patch_width == 0, 'Image dimensions must be divisible by the patch size.'

        num_patches = (image_height // patch_height) * (image_width // patch_width)
        patch_dim = channels * patch_height * patch_width
        assert pool in {'cls', 'mean'}, 'pool type must be either cls (cls token) or mean (mean pooling)'

        # step 1: creating the patches

        self.to_patch_embedding = nn.Sequential(
            nn.Conv2d(in_channels=channels,
                                 out_channels=patch_dim,
                                 kernel_size=3,
                                 stride=1,
                                 padding=1),nn.GELU(),nn.Conv2d(in_channels=patch_dim,out_channels=dim,kernel_size=patch_size,stride=patch_size,padding=0),nn.Flatten(start_dim=2, # only flatten the feature map dimensions into a single vector
                                  end_dim=3))
        
       # step 2: create the position embedding : 256 (number of patches) + 1 (class_token)
        
        self.pos_embedding = nn.Parameter(torch.randn(1,num_patches + 1, dim),requires_grad=True)
 
        self.dropout = nn.Dropout(emb_dropout)

        # step 3: create the transformer block
        self.transformer = Transformer(self.dim, depth, heads, dim_head, mlp_dim, shape, dropout)

        self.pool = pool
        self.to_latent = nn.Identity()
        # step 4: create the mlp_head
        self.mlp_head = nn.Sequential(
            nn.LayerNorm(self.dim),
            nn.Linear(self.dim, self.dim))
        # step 5: create the classifier to get output classes
        self.classifier = nn.Sequential(
            nn.LayerNorm(normalized_shape=self.dim),
            nn.Linear(in_features=dim, 
                      out_features=num_classes,bias=True))
        
        

    def forward(self, img):
        x = self.to_patch_embedding(img)
        x = x.permute(0,2,1)
        #print("after patch embedding",x.shape)
        b, n, d = x.shape
        batch_size=x.shape[0]

        # Create the class token embedding as a learnable parameter that shares the same size as the embedding dimension (D)
        self.cls_token = nn.Parameter(torch.randn(b,1, self.dim),requires_grad=True)

        #  Prepend class token embedding to patch embedding

        x = torch.cat((self.cls_token, x), dim=1)
        x = x + self.pos_embedding

        x = self.dropout(x)
        #print("before transformer",x.shape)
        x = self.transformer(x)
        #print("after transformer",x.shape)
        
        #x = x.mean(dim = 1) if self.pool == 'mean' else x[:, 0]
        #print("before latent",x.shape)
        x = self.to_latent(x[:,0])
        #print(x.shape)
        #x = x.permute(1,0)
        x = self.mlp_head(x)
        #print("x[:,0]",x[:,0].shape)
        x = self.classifier(x)
        return x
